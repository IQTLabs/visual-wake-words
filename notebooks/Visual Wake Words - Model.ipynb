{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Better Visual Wake Words Dataset\n",
    "\n",
    "## Introduction\n",
    "This notebook trains and evaluates models for detecting whether an object is in an image. A custom dataset is built using images from the COCO Dataset. It is based on the Tensorflow Lite Micro walk through on building a person detector. A few changes to the underlying code have been made to correct bugs and make it easier to run. The original directions are available here:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md\n",
    "\n",
    "More information on running the model that gets generated, can be found here: \n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection\n",
    "\n",
    "## Building the dataset\n",
    "In order to train a person detector model, we need a large collection of images that are labeled depending on whether or not they have people in them. The ImageNet one-thousand class data that's widely used for training image classifiers doesn't include labels for people, but luckily the COCO dataset does. You can also download this data without manually registering too, and Slim provides a convenient script to grab it automatically.\n",
    "\n",
    "This is a large download, about 40GB, so it will take a while and you'll need to make sure you have at least 100GB free on your drive to allow space for unpacking and further processing. The argument to the script is the path that the data will be downloaded to. If you change this, you'll also need to update the commands below that use it.\n",
    "\n",
    "The dataset is designed to be used for training models for localization, so the images aren't labeled with the \"contains a person\", \"doesn't contain a person\" categories that we want to train for. Instead each image comes with a list of bounding boxes for all of the objects it contains. \"Person\" is one of these object categories, so to get to the classification labels we want, we have to look for images with bounding boxes for people. To make sure that they aren't too tiny to be recognizable we also need to exclude very small bounding boxes. Slim contains a script to convert the bounding box into labels.\n",
    "\n",
    "Don't be surprised if this takes up to twenty minutes to complete. When it's done, you'll have a set of TFRecords in coco/processed holding the labeled image information. This data was created by Aakanksha Chowdhery and is known as the Visual Wake Words dataset. It's designed to be useful for benchmarking and testing embedded computer vision, since it represents a very common task that we need to accomplish with tight resource constraints. We're hoping to see it drive even better models for this and similar tasks."
   ]
  },
  {
   "source": [
    "## Model Configuration\n",
    "The following parameters are used to configure the model that will be generated. A Movilenet V1 model architecture will be used. \n",
    "\n",
    "- **DATA_DIR** This is the directory where the COCO Dataset is downloaded and stored and the training/eval data TFRecords are created. The default for the Docker container is: /tf/dataset\n",
    "- **TRAINING_DIR** This is the directory where model checkpoints are stored during training, and TFlite models are stored after conversion. The default for the Docker container is: /tf/training\n",
    "- **TRAINING_NAME** This is the name of the directory used in the Training Dir for the current model.\n",
    "- **CLASS_OF_INTEREST** This is the class of object, from the coco dataset that a detector will be built for.\n",
    "- **IMAGE_SIZE** This is the size of the image that will be used. The MobileNet V1 Architecture can support the following image sizes: 96, 128, 160, 192, 224\n",
    "- **USE_GRAYSCALE** Whether grayscale or color images should be used. It can be: True or False\n",
    "- **SMALL_OBJECT_AREA_THRESHOLD** This is the minimum percentage an object's bounding box can be of the overall image area. The default is 0.005 or 0.5%. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DATA_DIR = /tf/dataset\n",
    "%env TRAINING_DIR = /tf/training\n",
    "%env TRAINING_NAME = vww_128_color_bicycle_005\n",
    "%env CLASS_OF_INTEREST=bicycle\n",
    "%env IMAGE_SIZE=128\n",
    "%env USE_GRAYSCALE=False\n",
    "%env SMALL_OBJECT_AREA_THRESHOLD=0.005"
   ]
  },
  {
   "source": [
    "### Building the dataset\n",
    "\n",
    "In order to train a detector model the class of interest, we need a large collection of images\n",
    "that are labeled depending on whether or not they have that class in them. The\n",
    "ImageNet one-thousand class data that's widely used for training image\n",
    "classifiers doesn't include labels for people, but luckily the\n",
    "[COCO dataset](http://cocodataset.org/#home) does.\n",
    "\n",
    "This is a large download, about 40GB, so it will take a while and you'll need\n",
    "to make sure you have at least 100GB free on your drive to allow space for\n",
    "unpacking and further processing. \n",
    "\n",
    "The dataset is designed to be used for training models for localization, so the\n",
    "images aren't labeled with the \"contains an object\", \"doesn't contain an object\"\n",
    "categories that we want to train for. Instead each image comes with a list of\n",
    "bounding boxes for all of the objects it contains. To make sure that objects aren't\n",
    "too tiny to be recognizable we also need to exclude very small bounding boxes.\n",
    "\n",
    "Don't be surprised if this takes up to twenty minutes to complete. When it's\n",
    "done, you'll have a set of TFRecords in `coco/processed` holding the labeled\n",
    "image information. This data was created by Aakanksha Chowdhery and is known as\n",
    "the [Visual Wake Words dataset](https://arxiv.org/abs/1906.05721). It's designed\n",
    "to be useful for benchmarking and testing embedded computer vision, since it\n",
    "represents a very common task that we need to accomplish with tight resource\n",
    "constraints. We're hoping to see it drive even better models for this and\n",
    "similar tasks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /tf/models/research/slim/download_and_convert_data.py \\\n",
    "    --dataset_name=visualwakewords \\\n",
    "    --dataset_dir=\"${DATA_DIR}\" \\\n",
    "    --foreground_class_of_interest=\"${CLASS_OF_INTEREST}\" \\\n",
    "    --small_object_area_threshold=${SMALL_OBJECT_AREA_THRESHOLD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "One of the nice things about using tf.slim to handle the training is that the parameters you commonly need to modify are available as command line arguments, so we can just call the standard train_image_classifier.py script to train our model.\n",
    "\n",
    "This will take a couple of days on a single-GPU v100 instance to complete all one-million steps, but you should be able to get a fairly accurate model after a few hours if you want to experiment early.\n",
    "\n",
    "- The checkpoints and summaries will the saved in the folder given in the --train_dir argument, so that's where you'll have to look for the results.\n",
    "\n",
    "- The --dataset_dir parameter should match the one where you saved the TFRecords from the Visual Wake Words build script.\n",
    "\n",
    "- The architecture we'll be using is defined by the --model_name argument. The 'mobilenet_v1' prefix tells the script to use the first version of MobileNet. We did experiment with later versions, but these used more RAM for their intermediate activation buffers, so for now we kept with the original. The '025' is the depth multiplier to use, which mostly affects the number of weight parameters, this low setting ensures the model fits within 250KB of Flash.\n",
    "- --preprocessing_name controls how input images are modified before they're fed into the model. The 'mobilenet_v1' version shrinks the width and height of the images to the size given in --train_image_size (in our case 96 pixels since we want to reduce the compute requirements). It also scales the pixel values from 0 to 255 integers into -1.0 to +1.0 floating point numbers (though we'll be quantizing those after training).\n",
    "\n",
    "- The --input_grayscale flag configures whether images are converted to grayscale during preprocessing.\n",
    "\n",
    "- The --learning_rate, --label_smoothing, --learning_rate_decay_factor, --num_epochs_per_decay, --moving_average_decay and --batch_size are all parameters that control how weights are updated during the the training process. Training deep networks is still a bit of a dark art, so these exact values we found through experimentation for this particular model. You can try tweaking them to speed up training or gain a small boost in accuracy, but we can't give much guidance for how to make those changes, and it's easy to get combinations where the training accuracy never converges.\n",
    "\n",
    "- The --max_number_of_steps defines how long the training should continue. There's no good way to figure out this threshold in advance, you have to experiment to tell when the accuracy of the model is no longer improving to tell when to cut it off. In our case we default to a million steps, since with this particular model we know that's a good point to stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /tf/models/research/slim/train_image_classifier.py \\\n",
    "    --train_dir=\"${TRAINING_DIR}/${TRAINING_NAME}\" \\\n",
    "    --dataset_name=visualwakewords \\\n",
    "    --dataset_split_name=train \\\n",
    "    --dataset_dir=\"${DATA_DIR}\"  \\\n",
    "    --model_name=mobilenet_v1_025 \\\n",
    "    --preprocessing_name=mobilenet_v1 \\\n",
    "    --train_image_size=${IMAGE_SIZE} \\\n",
    "    --use_grayscale=${USE_GRAYSCALE} \\\n",
    "    --save_summaries_secs=300 \\\n",
    "    --learning_rate=0.045 \\\n",
    "    --label_smoothing=0.1 \\\n",
    "    --learning_rate_decay_factor=0.98 \\\n",
    "    --num_epochs_per_decay=2.5 \\\n",
    "    --moving_average_decay=0.9999 \\\n",
    "    --batch_size=96 \\\n",
    "    --max_number_of_steps=1000000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the line duplication, this is just a side-effect of the way TensorFlow log printing interacts with Python. Each line has two key bits of information about the training process. The global step is a count of how far through the training we are. Since we've set the limit as a million steps, in this case we're nearly five percent complete. The steps per second estimate is also useful, since you can use it to estimate a rough duration for the whole training process. In this case, we're completing about four steps a second, so a million steps will take about 70 hours, or three days. The other crucial piece of information is the loss. This is a measure of how close the partially-trained model's predictions are to the correct values, and lower values are better. This will show a lot of variation but should on average decrease during training if the model is learning. Because it's so noisy, the amounts will bounce around a lot over short time periods, but if things are working well you should see a noticeable drop if you wait an hour or so and check back. This kind of variation is a lot easier to see in a graph, which is one of the main reasons to try TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The loss function correlates with how well your model is training, but it isn't a direct, understandable metric. What we really care about is how many people our model detects correctly, but to get calculate this we need to run a separate script.\n",
    "\n",
    "The important number here is the accuracy. It shows the proportion of the images that were classified correctly, which is 72% in this case, after converting to a percentage. If you follow the example script, you should expect a fully-trained model to achieve an accuracy of around 84% after one million steps, and show a loss of around 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint number to use\n",
    "Enter the checkpoint number you wish to use. Checkpoints are stored in three separate files, so the value should be their common prefix. For example if you have a checkpoint file called 'model.ckpt-5179.data-00000-of-00001', the prefix would be 'model.ckpt-5179'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CHECKPOINT_NUM=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /tf/models/research/slim/eval_image_classifier.py \\\n",
    "    --alsologtostderr \\\n",
    "    --checkpoint_path=\"${TRAINING_DIR}/${TRAINING_NAME}/model.ckpt-${CHECKPOINT_NUM}\" \\\n",
    "    --dataset_dir=\"${DATA_DIR}\" \\\n",
    "    --dataset_name=visualwakewords \\\n",
    "    --dataset_split_name=val \\\n",
    "    --model_name=mobilenet_v1_025 \\\n",
    "    --preprocessing_name=mobilenet_v1 \\\n",
    "    --use_grayscale=${USE_GRAYSCALE} \\\n",
    "    --train_image_size=${IMAGE_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the  Graph\n",
    "\n",
    "When the model has trained to an accuracy you're happy with, you'll need to convert the results from the TensorFlow training environment into a form you can run on an embedded device. As we've seen in previous chapters, this can be a complex process, and tf.slim adds a few of its own wrinkles too.\n",
    "\n",
    "### Exporting to a GraphDef protobuf file\n",
    "\n",
    "Slim generates the architecture from the model_name every time one of its scripts is run, so for a model to be used outside of Slim it needs to be saved in a common format. We're going to use the GraphDef protobuf serialization format, since that's understood by both Slim and the rest of TensorFlow.\n",
    "\n",
    "If this succeeds, you should have a new '${TRAINING_NAME}_graph.pb' file in your ${TRAINING_DIR}/${TRAINING_NAME} folder. This contains the layout of the operations in the model, but doesn't yet have any of the weight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /tf/models/research/slim/export_inference_graph.py \\\n",
    "    --alsologtostderr \\\n",
    "    --dataset_name=visualwakewords \\\n",
    "    --model_name=mobilenet_v1_025 \\\n",
    "    --image_size=${IMAGE_SIZE} \\\n",
    "    --use_grayscale=${USE_GRAYSCALE} \\\n",
    "    --output_file=\"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_graph.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the weights\n",
    "\n",
    "The process of storing the trained weights together with the operation graph is known as freezing. This converts all of the variables in the graph to constants, after loading their values from a checkpoint file. The command below uses a checkpoint from the millionth training step, but you can supply any valid checkpoint path. The graph freezing script is stored inside the main tensorflow repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py \\\n",
    "--input_graph=\"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_graph.pb\" \\\n",
    "--input_checkpoint=\"${TRAINING_DIR}/${TRAINING_NAME}/model.ckpt-${CHECKPOINT_NUM}\" \\\n",
    "--input_binary=true --output_graph=\"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_frozen.pb\" \\\n",
    "--output_node_names=MobilenetV1/Predictions/Reshape_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone -b r1.15 https://github.com/tensorflow/tensorflow\n",
    "! python /tf/notebooks/tensorflow/tensorflow/python/tools/freeze_graph.py \\\n",
    "! python /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py \\\n",
    "--input_graph=\"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_graph.pb\" \\\n",
    "--input_checkpoint=\"${TRAINING_DIR}/${TRAINING_NAME}/model.ckpt-${CHECKPOINT_NUM}\" \\\n",
    "--input_binary=true --output_graph=\"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_frozen.pb\" \\\n",
    "--output_node_names=MobilenetV1/Predictions/Reshape_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing and converting to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization is a tricky and involved process, and it's still very much an active area of research, so taking the float graph that we've trained so far and converting it down to eight bit takes quite a bit of code. The majority of the code is preparing example images to feed into the trained network, so that the ranges of the activation layers in typical use can be measured. This is done differently if color or grayscale images are used. We rely on the TFLiteConverter class to handle the quantization and conversion into the TensorFlow Lite flatbuffer file that we need for the inference engine. \n",
    "\n",
    "TFLite models are generated with both UInt8 and Int8 inputs and outputs. The [person_detection](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection) example uses UInt8 while the [person_detection_experimental](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection_experimental) uses Int8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated based on:\n",
    "- https://stackoverflow.com/questions/58775848/tflite-cannot-set-tensor-dimension-mismatch-on-model-conversion\n",
    "- https://github.com/tensorflow/tensorflow/issues/34720\n",
    "- https://github.com/tensorflow/tensorflow/issues/34720#issuecomment-567652143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import io\n",
    "import PIL\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "image_size = int(os.environ.get('IMAGE_SIZE'))\n",
    "use_grayscale = os.environ.get('USE_GRAYSCALE')\n",
    "training_name = os.environ.get('TRAINING_NAME')\n",
    "training_dir = os.environ.get('TRAINING_DIR')\n",
    "\n",
    "# generates a subset of the dataset, with grayscale images\n",
    "def representative_grayscale_dataset_gen():\n",
    "  image_size = int(os.environ.get('IMAGE_SIZE'))\n",
    "  data_dir = os.environ.get('DATA_DIR')\n",
    "  record_iterator = tf.python_io.tf_record_iterator(path=data_dir+\"/val.record-00000-of-00010\")\n",
    "\n",
    "  count = 0\n",
    "  for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    image_stream = io.BytesIO(example.features.feature['image/encoded'].bytes_list.value[0])\n",
    "    image = PIL.Image.open(image_stream)\n",
    "    image = image.resize((image_size, image_size))\n",
    "    image = image.convert('L')\n",
    "    array = np.array(image)\n",
    "    array = np.expand_dims(array, axis=2)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = ((array / 127.5) - 1.0).astype(np.float32)\n",
    "    yield([array])\n",
    "    count += 1\n",
    "    if count > 300:\n",
    "        break\n",
    "        \n",
    "        \n",
    "# generates a subset of the dataset, with color images\n",
    "def representative_color_dataset_gen():\n",
    "  image_size = int(os.environ.get('IMAGE_SIZE'))\n",
    "  data_dir = os.environ.get('DATA_DIR')\n",
    "  record_iterator = tf.python_io.tf_record_iterator(path=data_dir+\"/val.record-00000-of-00010\")\n",
    "\n",
    "  count = 0\n",
    "  for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    image_stream = io.BytesIO(example.features.feature['image/encoded'].bytes_list.value[0])\n",
    "    image = PIL.Image.open(image_stream)\n",
    "    image = image.resize((image_size, image_size))\n",
    "    image = image.convert('RGB') \n",
    "    array = np.array(image)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = ((array / 127.5) - 1.0).astype(np.float32)\n",
    "    yield([array])\n",
    "    count += 1\n",
    "    if count > 300:\n",
    "        break\n",
    "        \n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(training_dir+\"/\" + training_name + \"/\"  + training_name + \"_frozen.pb\",['input'], ['MobilenetV1/Predictions/Reshape_1'])\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if use_grayscale == \"True\":\n",
    "    converter.representative_dataset = representative_grayscale_dataset_gen\n",
    "    print(\"Using Graysclae\")\n",
    "else:\n",
    "    converter.representative_dataset = representative_color_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type =  tf.uint8\n",
    "tflite_quant_model = converter.convert()\n",
    "open(training_dir +\"/\" + training_name + \"/\" + training_name + \"_quantized-uint8.tflite\", \"wb\").write(tflite_quant_model)\n",
    "\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  \n",
    "converter.inference_output_type = tf.int8  \n",
    "tflite_quant_model = converter.convert()\n",
    "open(training_dir +\"/\" + training_name + \"/\" + training_name + \"_quantized-int8.tflite\", \"wb\").write(tflite_quant_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Converting into a C source file\n",
    "\n",
    "The converter writes out a file, but most embedded devices don't have a file\n",
    "system. To access the serialized data from our program, we have to compile it\n",
    "into the executable and store it in Flash. The easiest way to do that is to\n",
    "convert the file into a C data array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file as a C source file\n",
    "!xxd -i \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_quantized-uint8.tflite\" > \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_quantized_flat-uint8.cc\"\n",
    "!xxd -i \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_quantized-int8.tflite\" > \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_quantized_flat-int8.cc\""
   ]
  },
  {
   "source": [
    "You can now replace the existing person_detect_model_data.cc file with the\n",
    "version you've trained, and be able to run your own model on embedded devices."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Convert the model to work with Tensorflow JS\n",
    "The same trained model can also be used in browser based Web Apps, thanks to Tensorflow JS. This step converts the frozen model into a form that can be loaded in Javascript.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\\n",
    "    \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_frozen.pb\" \\\n",
    "    \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_web_model\"\n",
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --quantize_uint8 \\\n",
    "    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\\n",
    "    \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_frozen.pb\" \\\n",
    "    \"${TRAINING_DIR}/${TRAINING_NAME}/${TRAINING_NAME}_web_model_uint8\""
   ]
  },
  {
   "source": [
    "### BYO-Data Testing\n",
    "It is helpful to test against the types of images your TinyML device will be seeing in the real world. This script lets you bring in an arbitrary set of images, run the model against them and then visualize the results. The images should be saved as JPEG using the `.jpg` extension. While this scripte will resize an image, it may not use the best method possible. If you are using very large images, from a phone, it might be best to resize first."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import json\n",
    "\n",
    "#This is the directory, located inside\n",
    "directory = \"dataset/bicycle_capture_resize/test3\"\n",
    "\n",
    "# https://note.nkmk.me/en/python-pillow-square-circle-thumbnail/\n",
    "\n",
    "# Center crops an image to get the desired width and height\n",
    "def crop_center(pil_img, crop_width, crop_height):\n",
    "    img_width, img_height = pil_img.size\n",
    "    return pil_img.crop(((img_width - crop_width) // 2,\n",
    "                         (img_height - crop_height) // 2,\n",
    "                         (img_width + crop_width) // 2,\n",
    "                         (img_height + crop_height) // 2))\n",
    "\n",
    "# Makes a square image\n",
    "def crop_max_square(pil_img):\n",
    "    return crop_center(pil_img, min(pil_img.size), min(pil_img.size))\n",
    "\n",
    "image_size = int(os.environ.get('IMAGE_SIZE'))\n",
    "use_grayscale = os.environ.get('USE_GRAYSCALE')\n",
    "training_name = os.environ.get('TRAINING_NAME')\n",
    "training_dir = os.environ.get('TRAINING_DIR')\n",
    "if(use_grayscale == \"True\"):\n",
    "    print(\"Using grayscale\")\n",
    "\n",
    "# The Int8 version of the TFLite model will be used\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=training_dir +\"/\" + training_name + \"/\" + training_name + \"_quantized-int8.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "# It looks like it is impossible to batch inference with the TF1.0 TFLite interpertter: https://github.com/tensorflow/tensorflow/issues/38158\n",
    "# doing 1 by 1 instead of as a batch\n",
    "\n",
    "files = os.listdir(directory)\n",
    "if (use_grayscale == \"True\"):\n",
    "    raw_image_batch = np.empty((0, image_size, image_size), np.int8)\n",
    "else:\n",
    "    raw_image_batch = np.empty((0, image_size, image_size, 3), np.int8)\n",
    "    \n",
    "results = []\n",
    "images_with_object = 0\n",
    "for f in files:\n",
    "    if f.endswith(\".jpg\"):\n",
    "        if (use_grayscale == \"True\"):\n",
    "            im = Image.open(directory+\"/\"+f).convert('L')\n",
    "        else:\n",
    "            im = Image.open(directory+\"/\"+f)\n",
    "\n",
    "        # the input images are resized and square cropped to match the what the model expects\n",
    "        im_thumb = crop_max_square(im).resize((image_size, image_size), Image.LANCZOS)\n",
    "        input_data = np.array(im_thumb, dtype=np.uint8)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # The UInt8 version of the image is needed for plotting later on\n",
    "        raw_image_batch = np.append(raw_image_batch, input_data, axis=0)\n",
    "        \n",
    "        # The input image data needs to be converted from UInt8 to Int8 for use with the model\n",
    "        input_data = (input_data -128).astype(np.int8)\n",
    "\n",
    "\n",
    "        # If it is a grayscale image we need to fill out the last axis of the array because it is expecting that shape\n",
    "        if (use_grayscale == \"True\"):\n",
    "            input_data = np.expand_dims(input_data, axis=3)\n",
    "\n",
    "        tflite_interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        tflite_interpreter.invoke()\n",
    "        predictions = tflite_interpreter.get_tensor(output_details[0]['index']).flatten()\n",
    "\n",
    "        # parse the results for this image\n",
    "        if (predictions[1] > predictions[0]):\n",
    "            has_object = True\n",
    "            images_with_object += 1\n",
    "        else:\n",
    "            has_object = False\n",
    "\n",
    "        results.append({\"prediction\":has_object})\n",
    "        \n",
    "\n",
    "# how many images are there\n",
    "batch_size = np.size(raw_image_batch,axis=0)\n",
    "\n",
    "# the outputted plot is going to have 3 columns, so figure out how many rows there should be\n",
    "rows = math.ceil(batch_size/3)\n",
    "plt.figure(figsize=(6, rows*2 ))\n",
    "\n",
    "\n",
    "\n",
    "print(\"{} totals images, {} images were detected with object\".format(batch_size, images_with_object))\n",
    "\n",
    "# plot out each image, along with whether the object was detected in it\n",
    "for i in range(batch_size):\n",
    "  ax = plt.subplot( rows,3, i+1)\n",
    "  if (use_grayscale == \"True\"):\n",
    "    plt.imshow(raw_image_batch[i].astype(\"uint8\"),cmap='gray')\n",
    "  else:\n",
    "    plt.imshow(raw_image_batch[i].astype(\"uint8\"))\n",
    "\n",
    "  plt.title(results[i][\"prediction\"])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}