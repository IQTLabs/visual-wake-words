{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Distribution of Visual Wake Word Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysts: Luke Berndt and John Speed Meyers\n",
    "# Date: September 10, 2020\n",
    "# Note: This notebook is derived from work by Google Research on visual wake words. Luke and\n",
    "# John Speed customized it to do further analysis for a potential paper on the limitations\n",
    "# of the current visual wake words dataset and training process and proposed methods\n",
    "# for at least partially overcoming these limitations.\n",
    "# Analyses\n",
    "# #1: Create eight graphs, one for each category size threshold (4) and one for category type (2),\n",
    "# where each category's percentage (of total samples) is graphed via a vertical bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import contextlib2\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "import pandas as pd\n",
    "import six\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_image_count(annotations_file):\n",
    "\n",
    "  with tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "    groundtruth_data = json.load(fid)\n",
    "    images = groundtruth_data['images']\n",
    "    print(\"Total images: {}\".format(len(images)))\n",
    "\n",
    "\n",
    "def create_visual_wakeword_annotations(annotations_file,\n",
    "                                       small_object_area_threshold,\n",
    "                                       foreground_classes_of_interest):\n",
    "  \"\"\"Generate visual wakewords annotations file.\n",
    "\n",
    "  Loads COCO annotation json files to generate visualwakewords annotations file.\n",
    "\n",
    "  Args:\n",
    "    annotations_file: JSON file containing COCO bounding box annotations\n",
    "    visualwakewords_annotations_file: path to output annotations file\n",
    "    small_object_area_threshold: threshold on fraction of image area below which\n",
    "      small object bounding boxes are filtered\n",
    "    foreground_classes_of_interest: category from COCO dataset that is filtered by\n",
    "      the visual wakewords dataset\n",
    "  \"\"\"\n",
    "\n",
    "  # default object of interest is person\n",
    "  images_with_class = 0\n",
    "  images_without_class = 0\n",
    "  foreground_classes_of_interest_id_list = []\n",
    "\n",
    "  with tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "    groundtruth_data = json.load(fid)\n",
    "    images = groundtruth_data['images']\n",
    "    # Create category index\n",
    "    category_index = {}\n",
    "    for category in groundtruth_data['categories']:\n",
    "      if category['name'] in foreground_classes_of_interest:\n",
    "        foreground_classes_of_interest_id_list.append(category['id'])\n",
    "        category_index[category['id']] = category\n",
    "\n",
    "    # Create annotations index, a map of image_id to it's annotations\n",
    "    annotations_index = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list))\n",
    "\n",
    "    # structure is { \"image_id\": {\"objects\" : [list of the image annotations]}}\n",
    "    for annotation in groundtruth_data['annotations']:\n",
    "        annotations_index[annotation['image_id']]['objects'].append(annotation)\n",
    "    missing_annotation_count = len(images) - len(annotations_index)\n",
    "\n",
    "    # Create filtered annotations index\n",
    "    annotations_index_filtered = {}\n",
    "    for idx, image in enumerate(images):\n",
    "        annotations = annotations_index[image['id']]\n",
    "        annotations_filtered = _filter_annotations(\n",
    "            annotations, image, small_object_area_threshold,\n",
    "            foreground_classes_of_interest_id_list)\n",
    "        annotations_index_filtered[image['id']] = annotations_filtered\n",
    "\n",
    "        if annotations_filtered['label'] == 1:\n",
    "            images_with_class += 1\n",
    "        else:\n",
    "            images_without_class += 1\n",
    "    \n",
    "    # TODO: Return number of images too\n",
    "    return images_with_class, (images_with_class / len(images))\n",
    "\n",
    "\n",
    "def _filter_annotations(annotations, image, small_object_area_threshold,\n",
    "                        foreground_classes_of_interest_id_list):\n",
    "  \"\"\"Filters COCO annotations to visual wakewords annotations.\n",
    "\n",
    "  Args:\n",
    "    annotations: dicts with keys: {\n",
    "      u'objects': [{u'id', u'image_id', u'category_id', u'segmentation',\n",
    "                  u'area', u'bbox' : [x,y,width,height], u'iscrowd'}] } Notice\n",
    "                    that bounding box coordinates in the official COCO dataset\n",
    "                    are given as [x, y, width, height] tuples using absolute\n",
    "                    coordinates where x, y represent the top-left (0-indexed)\n",
    "                    corner.\n",
    "    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',\n",
    "      u'width', u'date_captured', u'flickr_url', u'id']\n",
    "    small_object_area_threshold: threshold on fraction of image area below which\n",
    "      small objects are filtered\n",
    "    foreground_classes_of_interest_id_list: list of the categories of COCO dataset which visual\n",
    "      wakewords filters\n",
    "\n",
    "  Returns:\n",
    "    annotations_filtered: dict with keys: {\n",
    "      u'objects': [{\"area\", \"bbox\" : [x,y,width,height]}],\n",
    "      u'label',\n",
    "      }\n",
    "  \"\"\"\n",
    "\n",
    "  objects = []\n",
    "  image_area = image['height'] * image['width']\n",
    "\n",
    "  for annotation in annotations['objects']:\n",
    "    normalized_object_area = annotation['area'] / image_area\n",
    "    category_id = int(annotation['category_id'])\n",
    "    # Filter valid bounding boxes\n",
    "    if category_id in foreground_classes_of_interest_id_list and \\\n",
    "        normalized_object_area > small_object_area_threshold:\n",
    "      objects.append({\n",
    "          u'area': annotation['area'],\n",
    "          u'bbox': annotation['bbox'],\n",
    "      })\n",
    "\n",
    "  label = 1 if objects else 0\n",
    "\n",
    "  return {\n",
    "      'objects': objects,\n",
    "      'label': label,\n",
    "  }\n",
    "\n",
    "def create_visual_wakeword_category_list(annotations_file):\n",
    "    \"\"\"Generate lists of super- and sub-categories.\n",
    "\n",
    "    Create list of all categories used in visual wake word analysis\n",
    "\n",
    "    Args:\n",
    "      annotations_file: JSON file containing COCO bounding box annotations\n",
    "      \n",
    "    Returns:\n",
    "      (list1, list2)\n",
    "        list1: All super-categories used for visual wake word models\n",
    "        list2: All sub-categories used for visual wake word models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store supercategories and subcategories\n",
    "    super_categories, sub_categories = [], []\n",
    "\n",
    "    with tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "        \n",
    "        groundtruth_data = json.load(fid)\n",
    "        \n",
    "        # Loop thru all types of categories\n",
    "        for entry in groundtruth_data['categories']:\n",
    "            \n",
    "            # Extract novel supercategories\n",
    "            if entry['supercategory'] not in super_categories:\n",
    "                super_categories.append(entry['supercategory'])\n",
    "                \n",
    "            # Extract subcategories    \n",
    "            sub_categories.append(entry['name'])\n",
    "            \n",
    "    return super_categories, sub_categories\n",
    "\n",
    "def create_sub2super_category_dict(annotations_file):\n",
    "    \"\"\"Generate dict mapping of sub to super categories.\n",
    "\n",
    "    This mapping enables the create_analytical_dataframe function\n",
    "    to have a dict of sub- to super-categories that then\n",
    "    makes it possible to make graphs that are really aggregations\n",
    "    of the sub-categories.\n",
    "\n",
    "    Args:\n",
    "      annotations_file: JSON file containing COCO bounding box annotations\n",
    "      \n",
    "    Returns:\n",
    "      (dict) keys are sub-categories and values are super-category\n",
    "    \"\"\"\n",
    "    \n",
    "    # Empty dictionary to store mappings of sub-category to super-category\n",
    "    sub2super_dict = {}\n",
    "\n",
    "    with tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "        \n",
    "        groundtruth_data = json.load(fid)\n",
    "        \n",
    "        # Loop thru all types of categories\n",
    "        for entry in groundtruth_data['categories']:\n",
    "            \n",
    "            # Key is sub-category, value is supercategory\n",
    "            sub2super_dict[entry['name']] = entry['supercategory'] \n",
    "            \n",
    "    return sub2super_dict\n",
    "\n",
    "def create_analytical_dataframe(category_list, annotations_file):\n",
    "    \"\"\"Generate pandas dataframe to store all results needed for graphs\n",
    "    \n",
    "    Luke Berndt and John Speed Meyers wanted to analyze the categories present\n",
    "    in the visual wake word dataset and determine at different threshold levels\n",
    "    what percentage of the images contain each category.\n",
    "    \n",
    "    Args:\n",
    "      category_list: pandas dataframe of categories to be analyzed\n",
    "    \n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for category in category_list:\n",
    "        category_results = []\n",
    "        for threshold in [.005, .01, .05, .1, .25, .5]:\n",
    "            image_count, fraction = create_visual_wakeword_annotations(train_annotations_file,\n",
    "                                                        threshold,\n",
    "                                                        category)\n",
    "            rows.append([category, threshold, image_count, fraction])\n",
    "    \n",
    "    # Convert list of lists to dataframe\n",
    "    df = pd.DataFrame(rows,\n",
    "                      columns=['category', 'threshold', 'image_count', 'fraction'])\n",
    "    \n",
    "    # Instantiate mapping of sub to super categories\n",
    "    sub2super_dict = create_sub2super_category_dict(annotations_file)\n",
    "    \n",
    "    # TODO: Create function that creates new column with super categories\n",
    "    # for each row\n",
    "    df['supercategory'] = df.apply(lambda row: sub2super_dict[row['category']], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_dataframe_to_graphs(df, category=\"category\", y=\"fraction\"):\n",
    "    \"\"\"Output vertical bar graphs by threshold.\n",
    "    \n",
    "    Output one graph per threshold. Each x axis is the categories\n",
    "    present in the categories variable. The y axis is the fraction.\n",
    "    \n",
    "    TODO: If supercategory is specified, do call to separate aggregation\n",
    "    function\n",
    "    \n",
    "    Args:\n",
    "      df - A dataframe output by create_analytical_dataframe\n",
    "      category - Whether to use category (or supercategory)\n",
    "      y - A string indicating the variable to use on the axis\n",
    "    \n",
    "    \"\"\"\n",
    "    # Loop thru each unique threshold value present in the dataframe\n",
    "    for threshold in df.threshold.unique():\n",
    "\n",
    "        # Filter dataframe to contain only values of particular threshold\n",
    "        # and sort in descending order\n",
    "        df_filtered = df[df.threshold == threshold].sort_values(by=y, ascending=False)\n",
    "        \n",
    "        df_filtered.plot.bar(x=category, y=y,\n",
    "                             rot=70, # Rotate x-axis labels\n",
    "                             title=\"Fraction or count of images at {} threshold\".format(threshold), # Graph title\n",
    "                             figsize=(20,5), # Plot size\n",
    "                             legend=False) # Turn legend off\n",
    "        plot.xticks(fontsize=11) # Size of category labels\n",
    "\n",
    "    plot.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_analytical_dataframe(['car'], train_annotations_file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset paths\n",
    "coco_dir = \"/tf/dataset\"\n",
    "train_annotations_file = os.path.join(coco_dir, 'coco_dataset', 'annotations',\n",
    "                                        'instances_train2014.json')\n",
    "val_annotations_file = os.path.join(coco_dir, 'coco_dataset', 'annotations',\n",
    "                                        'instances_val2014.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_image_count(train_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categories for later analysis\n",
    "super_categories, sub_categories = create_visual_wakeword_category_list(train_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset for use in graphs below\n",
    "df = create_analytical_dataframe(sub_categories, train_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_category =  df['category']=='person'\n",
    "df_category = df[is_category]\n",
    "print(df_category)\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This graph making capability doesn't work for the \n",
    "# super categories yet.\n",
    "convert_dataframe_to_graphs(df, y=\"image_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}